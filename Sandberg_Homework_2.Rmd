---
title: "STAT 5361 - Homework 2"
# subtitle: "possible subtitle goes here"
author:
  - Kristen Sandberg^[<kristen.sandberg@uconn.edu>; M.S. in Applied Financial Mathematics,
    Department of Mathematics, University of Connecticut.]
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
papersize: letter
fontsize: 12pt
bibliography: template.bib
biblio-style: datalab
keywords: Template, R Markdown, bookdown, Data Lab
# keywords set in YAML header here only go to the properties of the PDF output
# the keywords that appear in PDF output are set in latex/before_body.tex
output:
  #bookdown::pdf_document2
  bookdown::html_document2
#abstract: |
#    This homework assignment is based on Chapter 2: Optimization and Solving Nonlinear Equations.
---



```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## some utility functions, see the source code for details
source("utils_template.R")

## specify the packages needed
pkgs <- c("splines2", "DT", "webshot", "leaflet", "graphics")
need.packages(pkgs)

## external data can be read in by regular functions,
## such as read.table or load

## get output format in case something needs extra effort
outFormat <- knitr::opts_knit$get("rmarkdown.pandoc.to")
## "latex" or "html"

## for latex and html output
isHtml <- identical(outFormat, "html")
isLatex <- identical(outFormat, "latex")
latex <- ifelse(isLatex, '\\LaTeX\\', 'LaTeX')

## specify global chunk options
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, dpi = 300,
                      out.width = "90%", fig.align = "center")

```


### Problem 1 {-}

The Cauchy$(\theta, 1)$ distribution has probability density 
\begin{align}
    p(x;\theta) = \dfrac{1}{\pi[1 + (x - \theta)^2]}, \,\, x \in \mathbb{R}, \,\, \theta \in \mathbb{R}.
    (\#eq:probdf)
\end{align}

#### Problem 1(a) {-}
Let $x_1, \ldots, x_n$ be an i.i.d. sample and $l(\theta)$ the log-likelihood of $\theta$ based on the sample.  Show that
\begin{align}
    l(\theta)  &= -n\ln{\pi} - \sum_{i=1}^{n} \ln{[1 + (x - \theta)^2]}, \\
    l'(\theta) &= -2 \sum_{i=1}^{n} \dfrac{\theta - x_i}{1 + (x - \theta)^2}, \\
    l''(\theta) &= -2 \sum_{i=1}^{n} \dfrac{1 - (\theta - x_i)^2}{[1 + (x - \theta)^2]^2}, \\
    I(\theta) &= n \int \dfrac{\{p'(x)\}^2}{p(x)}dx = \dfrac{4n}{\pi} \int_{-\infty}^{\infty} \dfrac{x^2 dx}{(1 + x^2)^3} = \dfrac{n}{2}.
    (\#eq:show1a)
\end{align}

#### Solution 1(a) {-}


#### Problem 1(b) {-}
Suppose that the observed sample is 
```{r define_x, echo = TRUE, eval=TRUE}
x <- c(1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44, 3.29, 3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)
```
Graph the log-likelihood  function. Find the MLE for $\theta$ using the Newton-Raphson method. Try all the following starting points: ???11, ???1, 0, 1.5, 4, 4.7, 7, 8, and 38.  Compare your results. Is the sample mean a good starting point?

#### Solution 1(b) {-}


#### Problem 1(c) {-}
Apply fixed-point iterations using $G(x) = \alpha l'(\theta) + \theta$, with scaling choices of $\alpha \in \{1, 0.64, 0.25\}$ and initial value ???1. Try the same starting points as above.

#### Solution 1(c) {-}


#### Problem 1(d) {-}
(d) First use Fisher scoring to find the MLE for $\theta$, then refine the estimate by running Newton-Raphson method. Try the same starting points as above.

#### Solution 1(d) {-}




```{r sources, echo = TRUE, message = FALSE, warning = FALSE}
## Some monte_carlo_cdf functions, see the source code for details
# set.seed(213)
# source("monte_carlo_function_approx_normal_cdf.R")
# source("monte_carlo_function_table.R")
```

(ref:Estimation) Comparison of Monte-Carlo estimated value and the actual values of Standard Normal CDF at different values of $t$.

```{r estimation, echo = TRUE}
# knitr::kable(monte_carlo_table(), booktabs = TRUE,
#              caption = '(ref:Estimation)')
```


<!-- (ref:cap-Boxplots1) Boxplots to show sample distribution of Monte-Carlo estimator. -->

```{r Boxplots1, echo = TRUE, fig.cap = "(ref:cap-Boxplots1)", fig.width = 8}
## some monte_carlo_cdf functions, see the source code for details
# set.seed(213)
# source("monte_carlo_function_100_simulations.R")
# source("monte_carlo_plot_results.R")
```

Further, the difference between the estimated value and the true value are also calculated for each of the 100 samples and are plotted using box plots to make further inferences. The box plots for bias should be distributed around 0 for unbiased estimators.The box plots for bias of Monte-Carlo simulation estimators of Standard Normal CDF for given values of $t$ and $n$ are visualized in the following figure \@ref(fig:Boxplots2)

<!-- (ref:cap-Boxplots2) Boxplots to show sample distribution of Bias. -->

```{r Boxplots2, echo = TRUE, fig.cap = "(ref:cap-Boxplots1)", fig.width = 8}
## some monte_carlo_cdf functions, see the source code for details
# source("monte_carlo_bias_box_plot.R")
```

### Conclusion and Summary {-}

In summary, we observed that the monte-carlo simulation provides reasonably accurate estimates for the normal cdf function as seen in \@ref(tab:estimation). Further, the bias of the estimator is distributed around zero. It was also observed that, independent of value $t$, as n increases, the spread of bias decreases. If hypothesis testing has to be done to verify unbiasedness of the monte-carlo estimate for all $t$, larger n will ensure that the estimator is unbiased for all values of $t$. 

It can also be inferred, that for extreme $t$ values, even small values of $n$ will ensure unbiased estimates for normal cdf. The bias spread decreases for larger values of $t$ and is so less, even for small $n$.

### Acknowledgment {-}

I would like to thank Surya Eada for his collaboration on this assignment prior to when the groups were split up. 


### Reference {-}


[pandoc]: http://pandoc.org/
[pandocManual]: http://pandoc.org/MANUAL.html
[repo]: https://github.com/wenjie2wang/datalab-templates
[taskView]: https://cran.r-project.org/web/views/ReproducibleResearch.html
[shiny.io]: https://www.shinyapps.io/
[wenjie-stat.shinyapps]: https://wwenjie-stat.shinyapps.io/minisplines2
