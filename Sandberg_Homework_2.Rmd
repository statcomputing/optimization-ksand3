---
title: "STAT 5361 - Homework 2"
# subtitle: "possible subtitle goes here"
author:
  - Kristen Sandberg^[<kristen.sandberg@uconn.edu>; M.S. in Applied Financial Mathematics,
    Department of Mathematics, University of Connecticut.]
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
papersize: letter
fontsize: 12pt
bibliography: template.bib
biblio-style: datalab
keywords: Template, R Markdown, bookdown, Data Lab
# keywords set in YAML header here only go to the properties of the PDF output
# the keywords that appear in PDF output are set in latex/before_body.tex
output:
  #bookdown::pdf_document2
  bookdown::html_document2
#abstract: |
#    This homework assignment is based on Chapter 2: Optimization and Solving Nonlinear Equations.
---



```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
## some utility functions, see the source code for details
source("utils_template.R")

## specify the packages needed
pkgs <- c("splines2", "DT", "webshot", "leaflet", "graphics")
need.packages(pkgs)

## external data can be read in by regular functions,
## such as read.table or load

## get output format in case something needs extra effort
outFormat <- knitr::opts_knit$get("rmarkdown.pandoc.to")
## "latex" or "html"

## for latex and html output
isHtml <- identical(outFormat, "html")
isLatex <- identical(outFormat, "latex")
latex <- ifelse(isLatex, '\\LaTeX\\', 'LaTeX')

## specify global chunk options
knitr::opts_chunk$set(fig.width = 5, fig.height = 4, dpi = 300,
                      out.width = "90%", fig.align = "center")

```


### Problem 1 {-}

The Cauchy$(\theta, 1)$ distribution has probability density 
\begin{align}
    p(x;\theta) = \dfrac{1}{\pi[1 + (x - \theta)^2]}, \,\, x \in \mathbb{R}, \,\, \theta \in \mathbb{R}.
    (\#eq:probdf)
\end{align}

#### Problem 1(a) {-}
Let $x_1, \ldots, x_n$ be an i.i.d. sample and $l(\theta)$ the log-likelihood of $\theta$ based on the sample.  Show that
\begin{align}
    l(\theta)  &= -n\ln{\pi} - \sum_{i=1}^{n} \ln{[1 + (\theta - x_i)^2]}, \\
    l'(\theta) &= -2 \sum_{i=1}^{n} \dfrac{\theta - x_i}{1 + (\theta - x_i)^2}, \\
    l''(\theta) &= -2 \sum_{i=1}^{n} \dfrac{1 - (\theta - x_i)^2}{[1 + (\theta - x_i)^2]^2}, \\
    I(\theta) &= n \int \dfrac{\{p'(x)\}^2}{p(x)}dx = \dfrac{4n}{\pi} \int_{-\infty}^{\infty} \dfrac{x^2 dx}{(1 + x^2)^3} = \dfrac{n}{2}.
    (\#eq:show1a)
\end{align}

#### Solution 1(a) {-}
Let $x_1, \ldots, x_n$ be an i.i.d. sample and $l(\theta)$ the log-likelihood of $\theta$ based on the sample.  Observe,
\begin{align}
    l(\theta) &= \ln{L(\theta)} \\
    &= \ln{\left(\prod_{i=1}^{n} \left(\dfrac{1}{\pi[1+(x_i - \theta)^2]} \right) \right)} \\
    &= \ln{\left(\prod_{i=1}^{n} \left(\pi[1+(x_i - \theta)^2] \right)^{-1} \right)} \\    
    &= \sum_{i=1}^{n} \ln{\left(\pi[1+(x_i - \theta)^2]\right)^{-1}} \\
    &= \sum_{i=1}^{n} \left[\ln{(1)} - \ln{\left(\pi[1+(x_i - \theta)^2]\right)} \right] \\
    &= \sum_{i=1}^{n} \left[0 - \ln{\left(\pi[1+(x_i - \theta)^2]\right)} \right] \\
    &= \sum_{i=1}^{n} \left[(- \ln{(\pi)}) -\ln{\left(1+(x_i - \theta)^2\right)} \right] \\
    &= -n\ln{\pi} - \sum_{i=1}^{n} \ln{[1 + (\theta - x_i)^2]}.
    (\#eq:show1a1)
\end{align}

Next, we have
\begin{align}
    l'(\theta) &= 0 - \sum_{i=1}^{n} \left(\dfrac{1}{1+(\theta - x_i)^2} \right)\left(2(\theta - x_i)(1) \right) \\
    &= -2 \sum_{i=1}^{n} \dfrac{\theta - x_i}{1 + (\theta - x_i)^2}.
    (\#eq:show1a2)
\end{align}

Then for the second derivative we have,
\begin{align}
    l''(\theta) &= -2 \sum_{i=1}^{n} \dfrac{[1 + (\theta - x_i)^2](1) - (\theta - x_i)[2(\theta - x_i)(1)]}{[1 + (\theta - x_i)^2]^2} \\
    &= -2 \sum_{i=1}^{n} \dfrac{1 + (\theta - x_i)^2 - 2(\theta - x_i)^2}{[1 + (\theta - x_i)^2]^2} \\
    &= -2 \sum_{i=1}^{n} \dfrac{1 - (\theta - x_i)^2}{[1 + (\theta - x_i)^2]^2}.
    (\#eq:show1a3)
\end{align}

Lastly, 
\begin{align}
I(\theta) &= \mathbb{E}\left[\left(l'(\theta) \right)^2 \right] \\
  &= \mathbb{E}\left[\left(-2 \sum_{i=1}^{n} \dfrac{\theta - x_i}{1 + (\theta - x_i)^2}\right)^2\right] \\
  &= 4\mathbb{E}\left[\sum_{i=1}^{n} \dfrac{(\theta - x_i)^2}{[1 + (\theta - x_i)^2]^2}\right] \\
  &= \dfrac{4n}{\pi} \int \dfrac{(\theta - x)^2}{[1 + (\theta - x)^2]^2} \dfrac{1}{1+(\theta - x)^2} dx \\
  &= \dfrac{4n}{\pi} \int \dfrac{(\theta - x)^2}{[1 + (\theta - x)^2]^3} dx \\
  &= \dfrac{4n\pi}{\pi^2} \int \dfrac{(\theta - x)^2}{[1 + (\theta - x)^2]^4}[1 + (\theta - x)^2] dx \\
  &= \dfrac{4n}{\pi^2} \int \dfrac{(\theta - x)^2}{[1 + (\theta - x)^2]^4}\left(\pi[1 + (\theta - x)^2]\right) dx \\
  &= n \int \dfrac{\{p'(x)\}^2}{p(x)}dx 
    (\#eq:show1a4)
\end{align}

This means, when $u = \theta - x$ and $du=dx$, we have
\begin{align}
I(\theta) &= n \int \dfrac{\{p'(x)\}^2}{p(x)}dx \\
  &= \dfrac{4n}{\pi^2} \int_{-\infty}^{\infty} \dfrac{u^2}{[1 + u^2]^4}\left(\dfrac{1}{\pi[1 + u^2]}\right)^{-1} du \\
  &= n \int_{-\infty}^{\infty} \dfrac{4\pi u^2(1+u^2)}{\pi^2[1 + u^2]^4}du \\
  &= \dfrac{4n}{\pi} \int_{-\infty}^{\infty} \dfrac{u^2 du}{(1 + u^2)^3}.
    (\#eq:show1a4b)
\end{align}

Thus, if we substitute $u = \tan{(\theta)}$ and $du = sec^2{(\theta)}$, we have
\begin{align}
I(\theta) &= \dfrac{4n}{\pi} \int_{-\infty}^{\infty} \dfrac{u^2 du}{(1 + u^2)^3}  \\
  &= \dfrac{4n}{\pi} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \dfrac{\tan^2{\theta}}{(1+\tan^2{\theta})^3}\sec^2{\theta}d\theta \\
  &= \dfrac{4n}{\pi} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \dfrac{\tan^2{\theta}}{(\sec^2{\theta})^3}\sec^2{\theta}d\theta \\
  &= \dfrac{4n}{\pi} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \dfrac{\tan^2{\theta}}{(\sec^2{\theta})^2}d\theta \\
  &= \dfrac{4n}{\pi} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \dfrac{\sin^2{\theta}}{\cos^2{\theta}}(\cos^2{\theta})^2d\theta \\
  &= \dfrac{4n}{\pi} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} \sin^2{\theta}\cos^2{\theta}d\theta \\
  &= \dfrac{4n}{\pi} \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}} (1-\cos^2{\theta})\cos^2{\theta}d\theta \\
  &= \dfrac{4n}{\pi} \left[\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos^2{\theta}d\theta - \int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos^4{\theta}d\theta \right] \\
  &= \dfrac{4n}{\pi}\left[\dfrac{1}{2}\theta + \dfrac{1}{4}\sin{2\theta} - \frac{1}{4}\cos^3{\theta}\sin{\theta}\right]\Big|_{-\frac{\pi}{2}}^{\frac{\pi}{2}} - \dfrac{3}{4}\int_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos^2{\theta}d\theta \\
   &= \dfrac{4n}{\pi}\left[\dfrac{1}{2}\theta + \dfrac{1}{4}\sin{2\theta} - \frac{1}{4}\cos^3{\theta}\sin{\theta}\right]\Big|_{-\frac{\pi}{2}}^{\frac{\pi}{2}} - \dfrac{3}{4}\left(\dfrac{1}{2}\theta + \dfrac{1}{4}\sin{2\theta} \right)\Big|_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos^2{\theta} \\
  &= \dfrac{4n}{\pi}\left[\dfrac{1}{4}\left(\dfrac{1}{2}\theta + \dfrac{1}{4}\sin{2\theta}\right) - \dfrac{1}{4}\cos^3{\theta}\sin{\theta}\right]\Big|_{-\frac{\pi}{2}}^{\frac{\pi}{2}}\cos^2{\theta} \\
  &= \dfrac{4n}{\pi}\left[\dfrac{1}{4}\left(\dfrac{1}{2}\left(\dfrac{\pi}{2} - \dfrac{-\pi}{2} \right)\right] + \left[\dfrac{1}{4}\sin{2\dfrac{\pi}{2}}\right] - \left[\dfrac{1}{4}\sin{2\dfrac{-\pi}{2}}\right]\right) - \dfrac{1}{4}\left[\cos^3{\dfrac{\pi}{2}}\sin{\dfrac{\pi}{2}} - \cos^3{\dfrac{-\pi}{2}}\sin{\dfrac{-\pi}{2}}\right] \\
  &= \dfrac{4n}{\pi}\left(\dfrac{\pi}{8} + 0\right) \\
  &= \dfrac{n}{2}.
    (\#eq:show1a4c)
\end{align}


#### Problem 1(b) {-}
Suppose that the observed sample is 
```{r define_x, echo = TRUE, eval=TRUE}
x_q1 <- c(1.77, -0.23, 2.76, 3.80, 3.47, 56.75, -1.34, 4.24, -2.44, 3.29, 3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)
```
Graph the log-likelihood  function. Find the MLE for $\theta$ using the Newton-Raphson method. Try all the following starting points: $-11, -1, 0, 1.5, 4, 4.7, 7, 8,$ and $38$.  Compare your results. Is the sample mean a good starting point?

#### Solution 1(b) {-}

The probability densiy function and log-likelihood  function are plotted below for the observed sample.  I built a function that finds the MLE for $\theta$ using the Newton-Raphson method.  The results are listed in the table below.  Most of the starting points converged to two maxima, one of which is the global maximum.  The sample mean was a good starting point because it did lead us to the MLE.

(ref:q1b) The MLE for $\theta$ using the Newton-Raphson method.

```{r q1b, echo = FALSE, eval = TRUE}
source('sandberg_assignment_2_MLE_finder_function.R')
x <- c(1.77, -0.23,  2.76, 3.80,  3.47, 56.75,  -1.34,  4.24, -2.44, 
       3.29,  3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)

starting_point <- c(-11, -1, 0, 1.5, 4, 4.7, 7, 8, 38, mean(x))
tolerance_err <- 0.000001

converges_to <- array()

l_prime_theta_t <- function(theta_t){ 
  -2 * sum((theta_t - x)/(1 + (theta_t - x)^2))
}
l_double_prime_theta_t <- function(theta_t){
  -2 * sum((1 - (theta_t - x)^2)/(1 + (theta_t - x)^2)^2)
}

for (k in 1:length(starting_point)){
  
  theta_t<-starting_point[k]
  
  converges_to[k] <- find_MLE(theta_t, l_prime_theta_t, l_double_prime_theta_t,tolerance_err,maximum_iterations = 100)
  
}  

# Create a table of the theta starting points and the values they converge to
MLE_table <- cbind(starting_point,converges_to)
colnames(MLE_table) <- c("Starting Point","Converges to")
knitr::kable(MLE_table, digits = 4, booktabs = TRUE,
             caption = '(ref:q1b)')

# Plot the probability density of the Cauchy distribution where theta is the sample mean
set_theta <- MLE_table[10,2]

all_x <-seq(-50,50,0.25)
prob_density <- array()

for (m in 1:length(all_x)){
  prob_density <- 1/(pi * (1 + (all_x - set_theta)^2))
}

pdf_title <- expression(paste("Probability Density of the Cauchy(", theta,",1) distribution"))
plot(all_x,prob_density,type="l",main = pdf_title,xlab="x")

# Plot the log-likelihood function
n_value <- length(x)
all_theta <-seq(-100,100,1)
l_theta_t <- array()

for (j in 1:length(all_theta)){
  l_theta_t[j] <- -n_value * log(pi) - sum(log(1+(all_theta[j] - x)^2))
}

log_title <- expression(paste("log-likelihood function,", l(theta)))
plot(all_theta,l_theta_t,type="l", main = log_title,xlab=expression(theta),ylab = expression(l(theta)))
```


#### Problem 1(c) {-}
Apply fixed-point iterations using $G(x) = \alpha l'(\theta) + \theta$, with scaling choices of $\alpha \in \{1, 0.64, 0.25\}$ and initial value $-1$. Try the same starting points as above.

#### Solution 1(c) {-}
When we apply the fixed-point iteration method with $G(\theta) = \alpha l'(\theta) + \theta$, with scaling choices of $\alpha \in \{1, 0.64, 0.25\}$, we find the following results in Table 2. 

(ref:q1c) The results of the fixed-point iterations using $G(x) = \alpha l'(\theta) + \theta$ where $\alpha \in \{1, 0.64, 0.25\}$.

```{r q1c, echo = FALSE, eval=TRUE}
# Kristen Sandberg
# Assignment 2

# Function to find the MLE given the starting theta, alpha, l', and a tolerance
# using the fixed-point iteration method

fixed_point_find_MLE <- function(starting_theta,alpha_value,l_prime,tolerance_epsilon){
  
  # Keep count of iterations so the while loop does not take too long
  count_iterations <- 1
  # Initialize the difference to be greater than the tolerance so the while loop works
  theta_difference <- tolerance_epsilon + 2
  # Begin with starting_theta
  theta_t_value <- starting_theta
  
  while ((theta_difference >= tolerance_epsilon) & count_iterations < 1000){
    
    theta_t_next <- alpha_value * l_prime(theta_t_value) + theta_t_value
    theta_difference <- abs(theta_t_next - theta_t_value)
    theta_t_value <- theta_t_next
    count_iterations <- count_iterations + 1
  }
  
  theta_t_value
  
}

x_q1 <- c(1.77, -0.23,  2.76, 3.80,  3.47, 56.75,  -1.34,  4.24, -2.44, 
       3.29,  3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)

starting_points <- c(-1, -11, 0, 1.5, 4, 4.7, 7, 8, 38)

alpha_n <- c(1, 0.64, 0.25)

l_prime_theta_t <- function(theta_t){ 
  -2 * sum((theta_t - x_q1)/(1 + (theta_t - x_q1)^2))
}

n_i <-1

starting_theta_q1c <- array()
alpha_i <- array()
converged_to_fp <- array()

for (k in 1:length(starting_points)){
  
  for (j in 1:length(alpha_n)){
  
    starting_theta_q1c[n_i] <- starting_points[k]
    alpha_i[n_i] <- alpha_n[j]
    converged_to_fp[n_i] <- fixed_point_find_MLE(starting_points[k], alpha_n[j], l_prime_theta_t, 0.00000001)
    n_i <- n_i + 1
    }
}  

# Create a table of the theta starting points and the values they converge to
MLE_fpi_table <- cbind(starting_theta_q1c,alpha_i,converged_to_fp)
colnames(MLE_fpi_table) <- c("Starting Point","Alpha","Attracted to Theta")

tab_len_1 <- floor(nrow(MLE_fpi_table) / 3)
tab_len_2 <- floor(nrow(MLE_fpi_table) / 3) * 2
table1 <- MLE_fpi_table[1:tab_len_1,]
table2 <- MLE_fpi_table[(tab_len_1 + 1):tab_len_2,]
table3 <- MLE_fpi_table[(tab_len_2+1):nrow(MLE_fpi_table),]
knitr::kable(list(table1, table2,table3), digits = 4, booktabs = TRUE,
             caption = '(ref:q1c)')
# knitr::kable(MLE_fpi_table, booktabs = TRUE,
             # caption = '(ref:q1c)')
```


#### Problem 1(d) {-}
First use Fisher scoring to find the MLE for $\theta$, then refine the estimate by running Newton-Raphson method. Try the same starting points as above.

#### Solution 1(d) {-}

We found that the starting points converged to two maxima, one of which was the global maximum, or MLE.

(ref:q1d) The results of using Fisher scoring to find the MLE from different starting points are shown in the table.

(ref:q1d2) The result of using the Newton-Raphson method to refine our results from the Fisher scoring method is shown in the table.

```{r q1d, echo = FALSE, eval=TRUE}
# Question 1d

x_q1d <- c(1.77, -0.23,  2.76, 3.80,  3.47, 56.75,  -1.34,  4.24, -2.44, 
       3.29,  3.71, -2.40, 4.53, -0.07, -1.05, -13.87, -2.53, -1.75)

starting_point <- c(-11, -1, 0, 1.5, 4, 4.7, 7, 8, 38, mean(x_q1d))

# Based on Solution to Question 1 part a
fisher_information <- length(x_q1d)/2

converges_to <- array()

for (k in 1:length(starting_point)){
  
  theta_t<-starting_point[k]
  
  for (i in 1:1000){
    
    l_prime_theta_t <- -2 * sum((theta_t - x_q1d)/(1 + (theta_t - x_q1d)^2))
    new_theta_t <- theta_t + (l_prime_theta_t / fisher_information)
    
    theta_t <- new_theta_t
  }
  
  converges_to[k] <- theta_t
  
}  

# Create a table of the theta starting points and the values they converge to
q1d_MLE_table <- cbind(starting_point,converges_to)
colnames(q1d_MLE_table) <- c("Starting Point","Converges to")

knitr::kable(q1d_MLE_table, digits = 5, booktabs = TRUE,
             caption = '(ref:q1d)')

# Find the mode of the points that the iterations converge to
calculate_mode <- function(vector_input) {
  uniqv_element <- unique(vector_input)
  uniqv_element[which.max(tabulate(match(vector_input, uniqv_element)))]
}
mode_is <- calculate_mode(q1d_MLE_table[,2])
l_of_mode_is <- -n_value * log(pi) - sum(log(1+(mode_is - x_q1d)^2))

# Plot the log-likelihood function
n_value <- length(x_q1d)
all_theta <-seq(-1,6,0.00025)
l_theta_t <- array()

for (j in 1:length(all_theta)){
  l_theta_t[j] <- -n_value * log(pi) - sum(log(1+(all_theta[j] - x_q1d)^2))
}

log2_title <- paste("log-likelihood function, at",round(mode_is,5))
plot(all_theta,l_theta_t,type="l", main = log2_title,xlab=expression(theta),ylab = expression(l(theta)))

# Refine using the Newton-Raphson method
tolerance_error <- 0.000001

l_prime_theta_t_newton <- function(theta_t){ 
  -2 * sum((theta_t - x_q1d)/(1 + (theta_t - x_q1d)^2))
}
l_double_prime_theta_t_newton <- function(theta_t){
  -2 * sum((1 - (theta_t - x_q1d)^2)/(1 + (theta_t - x_q1d)^2)^2)
}


converges_to_newton <- find_MLE(mode_is, l_prime_theta_t_newton, l_double_prime_theta_t_newton,tolerance_error)#,maximum_iterations = 100)
converges_to_newton <- formatC(converges_to_newton,digits=20)
mode_is <- formatC(mode_is,digits=20)
  

# Create a table of the theta starting points and the values they converge to
MLE_table_fisher_newton <- cbind(mode_is,converges_to_newton)
colnames(MLE_table_fisher_newton) <- c("Starting Point from Fisher Scoring","Converges to using Newton-Raphson")
knitr::kable(MLE_table_fisher_newton, booktabs = TRUE,
             caption = '(ref:q1d2)')
```

Using Fisher scoring, we found the MLE to be $\theta =$ `r mode_is`.  Once we refined this estimate using the Newton-Raphson method, we found the MLE to be $\theta =$ `r converges_to_newton`. 

#### Problem 1(e) {-}
Comment on the results from different methods (speed, stability, etc.).

#### Solution 1(e) {-}


### Problem 2 {-}

Consider the probability density with parameter $\theta$
\begin{align}
    p(x;\theta) = \dfrac{1-\cos{(x-\theta)}}{2\pi}, \,\, 0\leq x \leq 2\pi , \,\, \theta \in (-\pi,\pi).
    (\#eq:probdf2)
\end{align}

A random sample from the distribution is
```{r x_values, echo = TRUE, eval=TRUE}
x_q2 <- c(3.91, 4.85, 2.28, 4.06, 3.70, 4.04, 5.46, 3.53, 2.28, 1.96, 
       2.53, 3.88, 2.22, 3.47, 4.82, 2.46, 2.99, 2.54, 0.52)
```

#### Problem 2(a) {-}
What is the log-likelihood function of $\theta$ based on the sample? Graph the function between $-\pi$ and $\pi$.

#### Solution 2(a) {-}
The log-likelihood function of $\theta$ based on the given sample can be computed as follows:
\begin{align}
    l(\theta) &= \ln{\left[\prod_{i=1}^{n} \left(\dfrac{1-\cos{(x-\theta)}}{2\pi}\right)\right]} \\
    &= \sum_{i=1}^{n} \left[\ln{\left(\dfrac{1-\cos{(x-\theta)}}{2\pi}\right)}\right] \\
    &= \sum_{i=1}^{n} \left[\ln{\left(\dfrac{1}{2\pi}\right)} + \ln{\left(1-\cos{(x-\theta)}\right)}\right] \\
    &= n\ln{\left(\dfrac{1}{2\pi}\right)} + \sum_{i=1}^{n} \left[\ln{\left(1-\cos{(x-\theta)}\right)}\right] \\
    &= n\ln{\left(1\right)} - n\ln{\left(2\pi\right)} + \sum_{i=1}^{n} \left[\ln{\left(1-\cos{(x-\theta)}\right)}\right] \\
    &= - n\ln{\left(2\pi\right)} + \sum_{i=1}^{n} \left[\ln{\left(1-\cos{(x-\theta)}\right)}\right].
    (\#eq:q2aproof)
\end{align}

The graph of the log-likelihood function of $\theta$ is plotted from $[-\pi,\pi]$ in Figure \@ref(fig:q2a).
```{r q2a, echo = FALSE, eval=TRUE,fig.cap="\\label{fig.q2a} Log-likelihood function"}
# Plot the log-likelihood function
n_value <- length(x_q2)
all_theta <- seq(-pi,pi,0.0025)
l_theta_t <- array()

for (j in 1:length(all_theta)){
  l_theta_t[j] <- -n_value * log(2*pi) + sum(log(1-cos(x_q2-all_theta[j])))
}

y_name <- expression(paste("l(",theta,")=ln(L(",theta,"))",sep="",collapse=NULL))
title_name <- expression(paste("Log-likelihood function of ",theta,sep="",collapse=NULL))
plot(all_theta,l_theta_t,type="l",xlab=expression(theta),ylab = y_name,main = title_name)
```

#### Problem 2(b) {-}
Find the method-of-moments estimator of $\theta$. That is, the estimator $\hat{\theta}_{\text{moment}}$ is value of $\theta$ with
$\mathbb{E}[X | \theta] = \bar{x}$
where x is the sample mean. This means you have to first find the expression for $\mathbb{E}[X | \theta]$.

#### Solution 2(b) {-}
To find the method-of-moments estimator of $\theta$, we will  first find the expression for $\mathbb{E}[X | \theta]$, as shown below:

\begin{align}
    \mathbb{E}[X | \theta] &= \int_{0}^{2\pi} x p(x;\theta) dx \\
    &= \dfrac{1}{2\pi}\int_{0}^{2\pi} x (1-\cos{(x-\theta)}) dx \\
    &= \dfrac{1}{2\pi}\left[\int_{0}^{2\pi} x dx - \int_{0}^{2\pi}x\cos{(x-\theta)} dx\right] \\
    (\#eq:q2bproof1)
\end{align}

Using integration by parts, we find that

\begin{align}
    \mathbb{E}[X | \theta] &= \dfrac{1}{2\pi}\left[\dfrac{x^2}{2} \Big|_{0}^{2\pi} - x\sin{(x-\theta)}\Big|_{0}^{2\pi} + \int_{0}^{2\pi} \sin{(x-\theta)}dx \right] \\
     &= \left(\dfrac{4\pi^2}{4\pi} - 0\right) - \dfrac{1}{2\pi}\left[2\pi\sin{(2\pi-\theta)}-0\right] - \dfrac{1}{2\pi}\left[ \cos{(x-\theta)} \right]\Big|_{0}^{2\pi} \\
    &= \pi - \sin{(2\pi - \theta)} - \dfrac{1}{2\pi}\left[\cos{(2\pi-\theta)} - \cos{(0-\theta)} \right] \\
    &= \pi - \left[\sin{(2\pi)}\cos{(-\theta)}+\cos{(2\pi)}\sin{(-\theta)}\right] \\
    &- \frac{1}{2\pi}\left[\cos{(2\pi)}\cos{(-\theta)} - \sin{(2\pi)}\sin{(-\theta)}\right] + \frac{1}{2\pi}\left[\cos{(0)}\cos{(-\theta)} - \sin{(0)}\sin{(-\theta)}\right] \\
    &= \pi - \sin{(-\theta)} - \dfrac{1}{2\pi}\cos{(-\theta)} + \dfrac{1}{2\pi}\cos{(-\theta)} \\
    &= \pi - \sin{(-\theta)} \\
    &= \pi - \sin{(\theta)}.
    (\#eq:q2bproof2)
\end{align}

Now we set this equal to the sample mean, $\bar{x}$, and we have
\begin{align}
    \mathbb{E}[X | \theta] &= \pi - \sin{(\theta)} = \bar{x},
    (\#eq:q2bproof3)
\end{align}
so we subtract $\pi$ from both sides and we have 
\begin{align}
    -\sin{(\theta)} &= \bar{x} - \pi.
    (\#eq:q2bproof4)
\end{align}
Finally we multiply by $-1$ and take the inverse sine function of both sides and we find that 
\begin{align}
    \theta &= \arcsin{(\pi - \bar{x})}.
    (\#eq:q2bproof4)
\end{align}
Thus, the method-of-moments estimator of $\theta$, denoted $\hat{\theta}_{\text{moment}} = \arcsin{(\pi - \bar{x})}$.

```{r x_mean, echo = FALSE, eval=TRUE}
x_bar <- mean(x_q2)
theta_moment <- asin(pi - x_bar)
```

Given the observations, the sample mean of the given random sample is $\bar{x}=$ `r x_bar`.  The method-of-moments estimator of $\theta$ is $\hat{\theta}_{\text{moment}}=$ `r theta_moment`

#### Problem 2(c) {-}
Find the MLE for $\theta$ using the Newton-Raphson method with $\theta_0 = \hat{\theta}_{\text{moment}}$.

#### Solution 2(c) {-}
```{r q2cd, echo = FALSE, eval=TRUE}
# Question 2c and 2d
source('sandberg_assignment_2_MLE_finder_function.R')
l_prime_theta_t <- function(theta_t){
  -1 * sum((sin(x_q2 - theta_t)) / (1-cos(x_q2-theta_t)))
  }
l_double_prime_theta_t <- function(theta_t){
  -1 * sum(1 / (1-cos(x_q2-theta_t)))
  }

begin_theta <- theta_moment

converged_to_theta_tm <- find_MLE(begin_theta, l_prime_theta_t, l_double_prime_theta_t,0.00000001)

begin_theta <- 2.7

converged_to_theta_27 <- find_MLE(begin_theta, l_prime_theta_t, l_double_prime_theta_t,0.00000001)

begin_theta <- -2.7

converged_to_theta_neg27 <- find_MLE(begin_theta, l_prime_theta_t, l_double_prime_theta_t,0.00000001)
```
When we start with $\theta_0 = \hat{\theta}_{\text{moment}}=$ `r theta_moment` we find that the MLE for $\theta$ using the Newton-Raphson method is `r converged_to_theta_tm`.  

#### Problem 2(d) {-}
What solutions do you find when you start at $\theta_0 = -2.7$ and $\theta_0 = 2.7$?

#### Solution 2(d) {-}
When we start with $\theta_0 = -2.7$ we find that the MLE for $\theta$ using the Newton-Raphson method is `r converged_to_theta_neg27` and when $\theta_0 = 2.7$ the MLE is `r converged_to_theta_27`. 

#### Problem 2(e) {-}
Repeat the above using 200 equally spaced starting values between $-\pi$ and $\pi$. Partition the values into sets of attraction, That is, divide the set of starting values into separate groups, with each group corresponding to a separate unique outcome of the optimization.

#### Solution 2(e) {-}
Using 200 equally spaced starting values between $-\pi$ and $\pi$, I found the following unique outcomes and divided the set into groups that are attracted to the same outcome.

(ref:Attracts) The unique outcomes of the optimization using 200 equally spaced starting values between $-\pi$ and $\pi$.  The table lists the lower and upper bounds of values for $\theta$ that are attracted to each unique outcome.

```{r attracts, echo = FALSE, eval = TRUE}
# Question 2 part e
theta_200 <- seq(-pi,pi,length.out = 200)
attraction_outcome <- array()

for (i in 1:length(theta_200)){
  attraction_outcome[i] <- find_MLE(theta_200[i], l_prime_theta_t, l_double_prime_theta_t,0.00000001)
}

# Find unique values -- define unique based on rounding of 8 digits
attractors <- unique(round(attraction_outcome,8))

sets_of_attractors <- cbind(theta_200,attraction_outcome,round(attraction_outcome,8))

lowest_theta  <- array()
highest_theta <- array()

for (m in 1:length(attractors)){
  one_set_index <- which(sets_of_attractors[,3] == attractors[m])
  
  lowest_theta[m] <- min(sets_of_attractors[one_set_index,1])
  highest_theta[m] <- max(sets_of_attractors[one_set_index,1])
  
}

min_max_attract <- cbind(attractors,lowest_theta,highest_theta)
colnames(min_max_attract) <- c("Attracted to Theta", "Lowest Theta in Set", "Highest Theta in Set")


# q2_tab_len_1 <- floor(nrow(min_max_attract) / 3)
# q2_tab_len_2 <- floor(nrow(min_max_attract) / 3) * 2
# q2_table1 <- min_max_attract[1:q2_tab_len_1,]
# q2_table2 <- min_max_attract[(q2_tab_len_1 + 1):q2_tab_len_2,]
# q2_table3 <- min_max_attract[(q2_tab_len_2+1):nrow(min_max_attract),]
# knitr::kable(list(q2_table1, q2_table2, q2_table3), digits = 4, booktabs = TRUE,
#              caption = '(ref:Attracts)')
knitr::kable(min_max_attract, digits = 4, booktabs = TRUE,
             caption = '(ref:Attracts)')

```

### Problem 3 {-}
The counts of a floor beetle at various time points (in days) are given in a dataset.
```{r beetle_values, echo = TRUE, eval=TRUE}
beetles <- data.frame(
  days = c(0, 8, 28, 41, 63, 69, 97, 117, 135, 154),
  beetles = c(2, 47, 192, 256, 768, 896, 1120, 896, 1184, 1024))
```

A simple model for population growth is the logistic model given by
\begin{align}
    \dfrac{\text{d}N}{\text{d}t} &= rN\left(1-\dfrac{N}{K}\right),
    (\#eq:q3_1)
\end{align}
where $N$ is the population size, $t$ is time, $r$ is an unknown growth rate parameter, and $K$ is an unknown parameter that represents the population carrying capacity of the environment.  The solution to the differential equation is given by
\begin{align}
    N_t &= f(t) = \dfrac{KN_0}{N_0 + (K-N_0)\exp{(-rt)}}, 
    (\#eq:q3_2)
\end{align}
where $N_t$ denotes the population size at time $t$.  

#### Problem 3(a) {-}
Fit the population growth model to the beetles data using the Gauss-Newton approach, to minimize the sum of squared errors between model predictions and observed counts.

#### Solution 3(a) {-}


#### Problem 3(b) {-}
Show the contour plot of the sum of squared errors.

#### Solution 3(b) {-}


#### Problem 3(c) {-}
In many population modeling application, an assumption of lognormality is adopted. That is , we assume that $\log{N_t}$ are independent and normally distributed with mean $\log{f(t)}$ and variance $\sigma^2$. Find the maximum likelihood estimators of $r$, $K$, $\sigma^2$ using any suitable method of your choice. Estimate the variance your parameter estimates.

#### Solution 3(c) {-}



### Acknowledgment {-}

I would like to thank Surya Eada for his collaboration on problem 1a of this assignment prior to when the groups were split up. 


### Reference {-}


[pandoc]: http://pandoc.org/
[pandocManual]: http://pandoc.org/MANUAL.html
[repo]: https://github.com/wenjie2wang/datalab-templates
[taskView]: https://cran.r-project.org/web/views/ReproducibleResearch.html
[shiny.io]: https://www.shinyapps.io/
[wenjie-stat.shinyapps]: https://wwenjie-stat.shinyapps.io/minisplines2
